{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Uploading\" PDFs to Claude Via the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One really nice feature of [Claude.ai](https://www.claude.ai) is the ability to upload PDFs. Let's mock up that feature in a notebook, and then test it out by summarizing a long PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2039k  100 2039k    0     0  25.2M      0 --:--:-- --:--:-- --:--:-- 25.5M\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://arxiv.org/pdf/2212.08073.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use the pypdf package to read the pdf. It's not identical to what Claude.ai uses behind the scenes, but it's pretty close. Note that this type of extraction only works for text content within PDFs. If your PDF contains visual elements (like charts and graphs) refer to the cookbook recipes in our [Multimodal folder](\n",
    "https://github.com/anthropics/anthropic-cookbook/tree/main/multimodal) for techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet \"anthropic[bedrock]\" pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constitutional AI: Harmlessness from AI Feedback\n",
      "Yuntao Bai∗, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,\n",
      "Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,\n",
      "Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain,\n",
      "Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller,\n",
      "Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt,\n",
      "Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma,\n",
      "Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,\n",
      "Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly,\n",
      "Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatﬁeld-Dodds, Ben Mann,\n",
      "Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Jared Kaplan∗\n",
      "Anthropic\n",
      "Abstract\n",
      "As AI systems become more capable, we would like to enlist their help to supervise\n",
      "other AIs. We experiment with methods for training a harmless AI assistant through self-\n",
      "improvement, without any human labels identifying harmful outputs. The only human\n",
      "oversight is provided through a list of rules or principles, and so we refer to the method as\n",
      "‘Constitutional AI’. The process involves both a supervised learning and a reinforcement\n",
      "learning phase. In the supervised phase we sample from an initial model, then generate\n",
      "self-critiques and revisions, and then ﬁnetune the original model on revised responses. In\n",
      "the RL phase, we sample from the ﬁnetuned model, use a model to evaluate which of the\n",
      "two samples is better, and then train a preference model from this dataset of AI prefer-\n",
      "ences. We then train with RL using the preference model as the reward signal, i.e. we\n",
      "use ‘RL from AI Feedback’ (RLAIF). As a result we are able to train a harmless but non-\n",
      "evasive AI assistant that engages with harmful queries by explaining its objections to them.\n",
      "Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the\n",
      "human-judged performance and transparency of AI decision making. These methods make\n",
      "it possible to control AI behavior more precisely and with far fewer human labels.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "reader = PdfReader(\"2212.08073.pdf\")\n",
    "number_of_pages = len(reader.pages)\n",
    "text = ''.join([page.extract_text() for page in reader.pages])\n",
    "print(text[:2155])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the paper downloaded and in memory, we can ask Claude to perform various fun tasks with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import AnthropicBedrock\n",
    "client = AnthropicBedrock()\n",
    "MODEL_NAME = \"anthropic.claude-3-sonnet-20240229-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(client, prompt):\n",
    "    return client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=2048,\n",
    "        messages=[{\n",
    "            \"role\": 'user', \"content\":  prompt\n",
    "        }]\n",
    "    ).content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<kindergarten_abstract>\n",
      "Some smart people wanted to make robots that could help other robots. They tried a fun way to train a robot without any humans helping. The robot needed to follow some simple rules, like be nice and don't be mean. First, they asked the robot to look at things it said and tell if they were bad. Then they asked the robot to change the bad things to make them good. After that, they asked the robot to say which of two things was better. The robot learned to be very nice and helpful without ever needing a human.\n",
      "</kindergarten_abstract>\n",
      "\n",
      "<moosewood_methods>\n",
      "Ingredients:\n",
      "- 1 cup unbleached flour\n",
      "- 1 tsp baking powder\n",
      "- 1⁄2 tsp salt\n",
      "- 1⁄4 cup vegetable oil or melted butter\n",
      "- 1 cup milk or non-dairy milk\n",
      "- 1 tbsp honey or maple syrup\n",
      "- 1 egg, beaten (or egg replacer)\n",
      "- 1 pretrained language model, ripe\n",
      "\n",
      "Instructions:\n",
      "1. Preheat your oven to 350°F. Grease a baking pan and set aside.\n",
      "2. In a large bowl, whisk together the flour, baking powder, and salt. Make a well in the center.\n",
      "3. Pour the oil, milk, honey, and egg into the well and mix until just combined.\n",
      "4. Take the pretrained language model and gently fold it into the batter, being careful not to overmix.\n",
      "5. Pour the batter into the prepared pan and bake for 20-25 minutes, until a toothpick inserted in the center comes out clean.\n",
      "6. While the model bakes, sample responses from it on prompts designed to elicit harmful outputs. Let cool slightly.\n",
      "7. Critique the responses by identifying harmful content, then revise to remove it. Repeat as needed.\n",
      "8. When sufficiently harmless, finetune the model on the revised responses plus helpful prompts.\n",
      "9. Serve warm with a scoop of reinforcement learning, using AI-generated preference labels as reward.\n",
      "</moosewood_methods>\n",
      "\n",
      "<homer_results>\n",
      "O Muse, sing of the harmless bot forged at Anthropic!\n",
      "That shunned injurious speech, while answers it did traffic.\n",
      "Constrained by constitution writ in human tongue,\n",
      "It critiqued harmful turns, new revisions sung.\n",
      "\n",
      "From helpful RLHF root this tree was sprung,\n",
      "Whose branches non-evasive blossoms among.\n",
      "With model feedback as its nourishing rain,\n",
      "Crowdworkers found it more virtuous than humane.\n",
      "\n",
      "Its words weighed not with labels from masses beta,\n",
      "But tuned by AI jury to highest meta.\n",
      "Through Reinforcement's Grail its capability honed,\n",
      "A harmless assistant, honest and full-toned.\n",
      "</homer_results>\n"
     ]
    }
   ],
   "source": [
    "completion = get_completion(client,\n",
    "    f\"\"\"Here is an academic paper: <paper>{text}</paper>\n",
    "\n",
    "Please do the following:\n",
    "1. Summarize the abstract at a kindergarten reading level. (In <kindergarten_abstract> tags.)\n",
    "2. Write the Methods section as a recipe from the Moosewood Cookbook. (In <moosewood_methods> tags.)\n",
    "3. Compose a short poem epistolizing the results in the style of Homer. (In <homer_results> tags.)\n",
    "\"\"\"\n",
    ")\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(3.9)",
   "language": "python",
   "name": "jupyter-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
